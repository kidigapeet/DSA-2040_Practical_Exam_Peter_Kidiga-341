Markdown

# DSA 2040: Practical Exam - Data Warehousing and Data Mining

**Student Name:** Peter Kidiga
**Student ID:** 341
**Course:** DSA 2040 (FS 2025)
**Submission Date:** 10/12/2025

---

## ðŸ”° Introduction: What is this Project?

This project is a practical demonstration of two core concepts in data science: **Data Warehousing** and **Data Mining**. It was built as a final exam submission to solve real-world data problems using Python.

### The Two Main Goals:
1.  **Data Warehousing (Organizing Data):** Imagine a messy excel sheet of sales. We take that messy data, clean it up, and organize it into a structured "Data Warehouse" so businesses can easily answer questions like *"How much did we sell in the UK last Christmas?"*
2.  **Data Mining (Finding Patterns):** We use machine learning algorithms to find hidden patterns in data.
    * *Clustering:* Grouping flowers based on their size.
    * *Classification:* Teaching a computer to identify flower species.
    * *Market Basket Analysis:* Discovering shopping habits (e.g., *"People who buy diapers often buy beer"*).

---

## ðŸ“‚ Project Structure (Where is everything?)

Here is how the folders and files are organized.

```text
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Online Retail.xlsx       # The raw data file (Download from UCI)
â”‚   â””â”€â”€ retail_dw.db             # The database we create (The "Warehouse")
â”‚
â”œâ”€â”€ scripts/                     # The Python code files
â”‚   â”œâ”€â”€ projectcode           
|   |-- starschemasql

â”œâ”€â”€ images/                      # Charts and Diagrams generated by the code
â”‚   â”œâ”€â”€ star_schema_diagram.png  # Diagram of our database structure
â”‚   â”œâ”€â”€ cluster_visualization.png# Chart showing flower groups
â”‚   â””â”€â”€ decision_tree_viz.png    # Chart showing how the AI makes decisions
â”‚
â””â”€â”€ README.md                    # This file (The manual)
```
ðŸš€ Getting Started (How to Run It)
If you are new to Python, follow these steps to run the project on your computer.

Step 1: Install Required Tools
You need Python installed. Then, open your terminal (Command Prompt) and install the necessary libraries by typing:

```
pip install pandas numpy scikit-learn matplotlib seaborn mlxtend db-sqlite3 openpyxl
Step 2: Get the Data
```
Download the Online Retail Dataset from the UCI Repository.

Save the Excel file as Online Retail.xlsx inside the data/ folder (or next to your scripts).

ðŸ“Š Section 1: Data Warehousing (The Retail Store)
In this section, we build a system to track sales for a global retail company.

Task 1: The Design ("Star Schema")
Before writing code, we designed a blueprint called a Star Schema.

Concept: Think of a star. In the center, we have the Fact Table (numbers like Sales, Quantity). The points of the star are Dimension Tables (details like Customer Name, Product Category, Date).

Why? This structure separates the "Math" from the "Details," making the computer much faster at generating reports.

Visual: See images/star_schema_diagram.png.

Task 2: The Build ("ETL Process")
We wrote a script called etl_retail.py. It performs ETL:

Extract (E): Reads the raw Excel file.

Transform (T): Cleans the data.

Example: We removed refunds (negative numbers) and calculated Total Sales (Price Ã— Quantity).

Load (L): Saves the clean data into a SQLite database (retail_dw.db).


Task 3: The Analysis ("OLAP Queries")
Now that the data is clean, we use olap_queries.py to ask questions:

Roll-up: "Show me total sales by Country." (Zooming out).

Drill-down: "Show me UK sales broken down by month." (Zooming in).

Slice: "Show me only sales for the 'Electronics' category." (Cutting a slice).

python scripts/olap_queries.py

ðŸ§  Section 2: Data Mining (The AI Part)
In this section, we use the famous Iris Flower Dataset to perform Machine Learning.

Task 1: Preprocessing (Cleaning)
The script preprocessing_iris.py prepares the data.

Normalization: It scales the measurements (like petal length) so they are all between 0 and 1. This prevents large numbers from confusing the AI.

Visualization: It draws charts (boxplots and heatmaps) so we can see what the data looks like.

Task 2: Clustering (Grouping)
We use clustering_iris.py to perform K-Means Clustering.

Goal: Imagine you have a bucket of mixed flowers without labels. We want the computer to sort them into 3 piles based on how they look.

Result: The computer successfully grouped the flowers into 3 clusters that match the real species (Setosa, Versicolor, Virginica). We verified this using an "Elbow Curve" chart.


Task 3: Classification & Shopping Patterns
This final script, mining_iris_basket.py, does two things:

Part A: Classification (Teaching)

We trained a "Decision Tree" (a flowchart-like model) to predict the name of a flower based on its petal size.

Accuracy: The model achieved ~95% accuracy.

Part B: Market Basket Analysis (Shopping)

We generated fake shopping receipts (e.g., ['Milk', 'Bread', 'Diapers']).

We used the Apriori Algorithm to find rules.

Discovery: The algorithm found that "If a customer buys Diapers, they are likely to buy Beer". This is a classic example used by supermarkets to place items closer together.
